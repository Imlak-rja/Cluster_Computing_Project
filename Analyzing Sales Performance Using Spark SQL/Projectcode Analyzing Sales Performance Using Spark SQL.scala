Project:code: Analyzing Sales Performance Using Spark SQL

Context: A retail company wants to analyze its sales records to understand performance trends across products, regions, and time.
Tasks:
1.	Load the sales dataset into Spark and convert it to a DataFrame. 
2.	Register the DataFrame as a temporary SQL table. 
3.	Write a SQL query to find total revenue generated by each product category. 
4.	Calculate monthly sales totals across different regions. 
5.	Identify the top 3 best-selling products based on quantity sold. 
6.	Visualize monthly sales performance using a line chart. 
7.	Calculate the average price of products in each category (using a subquery). 
8.	Find the region with the highest total revenue (using a subquery). 
9.	Visualize the total revenue by product category using a bar chart.
10.	Find products with revenue greater than the average revenue.

# powershell Scala code 
steps: 
1.	Load the sales dataset into Spark and convert it to a DataFrame.

Scala>  import org.apache.spark.sql.SparkSession
 import org.apache.spark.sql.SparkSession

scala> val spark = SparkSession.builder().master("local[1]").appName("Sales Data Analysis").getOrCreate()
25/04/15 00:52:28 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.
spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@7da43bda

scala> val df = spark.read.option("header", true).csv("C:/Users/HP/Desktop/Cluster Computing Project/project_sales_data.csv")
df: org.apache.spark.sql.DataFrame = [OrderID: string, Product: string ... 6 more fields]


scala> df.printSchema()
root
 |-- OrderID: string (nullable = true)
 |-- Product: string (nullable = true)
 |-- Category: string (nullable = true)
 |-- Region: string (nullable = true)
 |-- Quantity: string (nullable = true)
 |-- Price: string (nullable = true)
 |-- OrderDate: string (nullable = true)
 |-- Revenue: string (nullable = true)


scala> df.show()
+--------+--------+-----------+------+--------+------+----------+-------+
| OrderID| Product|   Category|Region|Quantity| Price| OrderDate|Revenue|
+--------+--------+-----------+------+--------+------+----------+-------+
|f9b8ad97|  Travel|  Furniture| South|       3|339.34|23-08-2024|1018.02|
|370d5ac6|     Big|Electronics| North|       7|230.89|11-11-2024|1616.23|
|d01c7150|  Person|Electronics|  West|       7|274.65|12-12-2024|1922.55|
|7e0bb90b|   Civil|  Furniture| South|       4| 993.6|19-12-2024| 3974.4|
|1426d3bb|  Assume|Electronics|  West|       6|550.02|28-09-2024|3300.12|
|d9332742|   Major|Electronics|  West|       5|996.89|25-07-2024|4984.45|
|1bc18f45| Protect|Electronics| South|       8|692.74|05-07-2024|5541.92|
|dc09b188| Receive|   Clothing| South|       4|937.39|29-04-2024|3749.56|
|27ac9202|Southern|      Books| North|       3|679.08|08-10-2024|2037.24|
|bf488b81|    Call|   Clothing| North|       7|321.92|10-05-2024|2253.44|
|c61b4f26|   Admit|   Clothing|  West|       2|383.52|17-08-2024| 767.04|
|56b47f9c|   Stage|      Books| North|       6|912.68|31-08-2024|5476.08|
|02a5db86|    Read|Electronics| North|       2|929.88|20-07-2024|1859.76|
|0104a16f|   Other|Electronics|  East|       4|674.06|07-12-2024|2696.24|
|4d235488|   Phone|  Furniture| North|       2|188.11|28-04-2024| 376.22|
|1d6076da|Personal|   Clothing| South|       1|334.82|24-05-2024| 334.82|
|5f8420fb|    Away|      Books|  East|       8|260.14|22-12-2024|2081.12|
|5334824e|     She|  Furniture| North|       4|658.05|27-01-2025| 2632.2|
|ed8bd8c6| Improve|   Clothing|  East|       7|238.95|12-12-2024|1672.65|
|d6119d68|    Most|   Clothing|  East|       5|496.76|25-09-2024| 2483.8|
+--------+--------+-----------+------+--------+------+----------+-------+
only showing top 20 rows

2.	Register the DataFrame as a temporary SQL table. 

scala> df.createOrReplaceTempView("sales")

3.	Write a SQL query to find total revenue generated by each product category.

scala> val revenueByCategory = spark.sql(""" SELECT Category, SUM(Revenue) AS TotalRevenue FROM sales GROUP BY Category ORDER BY TotalRevenue DESC """)
revenueByCategory: org.apache.spark.sql.DataFrame = [Category: string, TotalRevenue: double]

scala> revenueByCategory.show()
+-----------+------------------+
|   Category|      TotalRevenue|
+-----------+------------------+
|   Clothing|126834.02999999998|
|      Books|121005.02999999998|
|Electronics|         111527.88|
|  Furniture|107058.15000000001|
+-----------+------------------+

4.	Calculate monthly sales totals across different regions.

scala> val monthlySales = spark.sql(""" SELECT Region, SUBSTRING(OrderDate, 4, 2) AS Month, SUM(Revenue) AS MonthlySales FROM sales GROUP BY Region, Month ORDER BY Region, Month """)
monthlySales: org.apache.spark.sql.DataFrame = [Region: string, Month: string ... 1 more field]

scala> monthlySales.show()
+------+-----+------------------+
|Region|Month|      MonthlySales|
+------+-----+------------------+
|  East|   01|          14822.77|
|  East|   02|            2986.0|
|  East|   03|            3900.8|
|  East|   04|           4472.16|
|  East|   05|15255.849999999999|
|  East|   06|          10955.31|
|  East|   07|            5797.0|
|  East|   08|            9768.0|
|  East|   09|16058.760000000002|
|  East|   10| 6644.400000000001|
|  East|   11|           1685.61|
|  East|   12|           7393.17|
| North|   01|           8337.66|
| North|   02|           10204.2|
| North|   03|          17020.13|
| North|   04|           3954.02|
| North|   05|24069.109999999997|
| North|   06|           2845.62|
| North|   07|          18751.86|
| North|   08|           37286.9|
+------+-----+------------------+
only showing top 20 rows


5.	Identify the top 3 best-selling products based on quantity sold. 

scala> val topProducts = spark.sql(""" SELECT Product, SUM(Quantity) AS TotalQuantity FROM sales GROUP BY Product ORDER BY TotalQuantity DESC LIMIT 3 """)
topProducts: org.apache.spark.sql.DataFrame = [Product: string, TotalQuantity: double]

scala> topProducts.show()
+-------+-------------+
|Product|TotalQuantity|
+-------+-------------+
|    Big|         24.0|
|  Field|         18.0|
|    Pay|         17.0|
+-------+-------------+

6.	Visualize monthly sales performance using a line chart.

Code (Scala and Python):
o	Scala (Saving to CSV):
 monthlySales.coalesce(1).write.option("header", "true").csv("C:/Users/HP/Desktop/Cluster Computing Project/monthly_sales.csv")

 
Python Code for Visualization:
import pandas as pd
import glob
import matplotlib.pyplot as plt

file_list = glob.glob("C:/Users/HP/Desktop/Cluster Computing Project/monthly_sales.csv/part-*.csv")

if file_list:
    df = pd.read_csv(file_list[0])
    monthly_totals = df.groupby("Month")["MonthlySales"].sum().reset_index()
    plt.figure(figsize=(10, 6))
    plt.plot(monthly_totals["Month"], monthly_totals["MonthlySales"], marker="o")
    plt.title("Total Monthly Sales Performance")
    plt.xlabel("Month")
    plt.ylabel("Total Monthly Sales")
    plt.grid(True)
    plt.show()
else:
    print("No part files found!")



7.	Calculate the average price of products in each category (using a subquery). 

scala> val avgPriceByCategory = spark.sql(""" SELECT Category, AVG(Price) AS AveragePrice FROM (SELECT Category, Price FROM sales) AS product_prices GROUP BY Category ORDER BY AveragePrice DESC """)
avgPriceByCategory: org.apache.spark.sql.DataFrame = [Category: string, AveragePrice: double]

scala> avgPriceByCategory.show()
+-----------+-----------------+
|   Category|     AveragePrice|
+-----------+-----------------+
|  Furniture|621.8505714285715|
|Electronics| 569.821142857143|
|      Books|545.9354545454546|
|   Clothing|493.2893478260869|
+-----------+-----------------+

8.	Find the region with the highest total revenue (using a subquery). 

scala> val highestRevenueRegion = spark.sql(""" SELECT Region, TotalRevenue FROM (SELECT Region, SUM(Revenue) AS TotalRevenue FROM sales GROUP BY Region) AS region_revenue WHERE TotalRevenue = (SELECT MAX(TotalRevenue) FROM (SELECT SUM(Revenue) AS TotalRevenue FROM sales GROUP BY Region) AS max_revenue) """)
highestRevenueRegion: org.apache.spark.sql.DataFrame = [Region: string, TotalRevenue: double]

scala> highestRevenueRegion.show()
+------+------------------+
|Region|      TotalRevenue|
+------+------------------+
| North|161605.90999999995|
+------+------------------+


9.	Visualize the total revenue by product category using a bar chart.

Code (Scala and Python):

o	Scala (Saving to CSV):
revenueByCategory.coalesce(1).write.option("header", "true").csv("C:/Users/HP/Desktop/Cluster Computing Project/revenue_by_category.csv")

Python Code for Visualization:
import pandas as pd
import glob
import matplotlib.pyplot as plt

file_list = glob.glob("C:/Users/HP/Desktop/Cluster Computing Project/revenue_by_category.csv/part-*.csv")

if file_list:
    df = pd.read_csv(file_list[0])
    plt.figure(figsize=(10, 6))
    plt.bar(df["Category"], df["TotalRevenue"])
    plt.title("Total Revenue by Product Category")
    plt.xlabel("Category")
    plt.ylabel("Total Revenue")
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.show()
else:
    print("No part files found!")


10.	Find products with revenue greater than the average revenue.

scala> val productsAboveAvgRevenue = spark.sql(""" SELECT Product, Revenue FROM sales WHERE Revenue > (SELECT AVG(Revenue) FROM sales) """)
productsAboveAvgRevenue: org.apache.spark.sql.DataFrame = [Product: string, Revenue: string]

scala> productsAboveAvgRevenue.show()
+-----------+-------+
|    Product|Revenue|
+-----------+-------+
|      Civil| 3974.4|
|     Assume|3300.12|
|      Major|4984.45|
|    Protect|5541.92|
|    Receive|3749.56|
|      Stage|5476.08|
|  Education|   2986|
|   Behavior|4536.75|
|     Anyone| 3050.6|
|       Wear|4706.91|
|Information|3183.18|
|  Determine|7038.48|
|   Behavior|5802.08|
|     Always|7698.64|
|     Myself|5047.26|
|   Indicate|8360.55|
|     Health|4895.45|
|     Before| 3860.1|
|Interesting|4472.16|
|         As|2945.07|
+-----------+-------+
only showing top 20 rows



#Python code: part-1 Monthly sales performance

import pandas as pd
import glob
import matplotlib.pyplot as plt

file_list = glob.glob("C:/Users/HP/Desktop/Cluster Computing Project/monthly_sales.csv/part-*.csv")

if file_list:
    df = pd.read_csv(file_list[0])
    monthly_totals = df.groupby("Month")["MonthlySales"].sum().reset_index()
    plt.figure(figsize=(10, 6))
    plt.plot(monthly_totals["Month"], monthly_totals["MonthlySales"], marker="o")
    plt.title("Total Monthly Sales Performance")
    plt.xlabel("Month")
    plt.ylabel("Total Monthly Sales")
    plt.grid(True)
    plt.show()
else:
    print("No part files found!")

# python code part-2 RevenueByCategory

import pandas as pd
import glob
import matplotlib.pyplot as plt

file_list = glob.glob("C:/Users/HP/Desktop/Cluster Computing Project/revenue_by_category.csv/part-*.csv")

if file_list:
    df = pd.read_csv(file_list[0])
    plt.figure(figsize=(10, 6))
    plt.bar(df["Category"], df["TotalRevenue"])
    plt.title("Total Revenue by Product Category")
    plt.xlabel("Category")
    plt.ylabel("Total Revenue")
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.show()
else:
    print("No part files found!")
